"""
LLM ëª¨ë¸ ì´ˆê¸°í™” ë° ê´€ë¦¬ ìœ í‹¸ë¦¬í‹°
OpenAI, Ollama ë“± ë‹¤ì–‘í•œ LLM ì§€ì›
"""

import os
from typing import Optional, Dict, Any, List
from typing import Type
from pydantic import BaseModel
from langchain_openai import ChatOpenAI
from langchain_ollama import ChatOllama
from langchain_core.language_models import BaseChatModel
from langchain_core.messages import BaseMessage, HumanMessage, SystemMessage, AIMessage


class LLMManager:
    """LLM ê´€ë¦¬ í´ë˜ìŠ¤"""

    def __init__(self):
        self._models: Dict[str, BaseChatModel] = {}
        self._default_model: Optional[str] = None

    def add_openai_model(
        self,
        name: str = "openai",
        model: str = "gpt-3.5-turbo",
        api_key: Optional[str] = None,
        temperature: float = 0.0,
        **kwargs
    ) -> BaseChatModel:
        """OpenAI ëª¨ë¸ ì¶”ê°€"""
        api_key = api_key or os.getenv("OPENAI_API_KEY")
        if not api_key:
            raise ValueError("OpenAI API í‚¤ê°€ í•„ìš”í•©ë‹ˆë‹¤. OPENAI_API_KEY í™˜ê²½ë³€ìˆ˜ë¥¼ ì„¤ì •í•˜ê±°ë‚˜ api_key ë§¤ê°œë³€ìˆ˜ë¥¼ ì œê³µí•˜ì„¸ìš”.")

        llm = ChatOpenAI(
            model=model,
            api_key=api_key,
            temperature=temperature,
            **kwargs
        )

        self._models[name] = llm
        if self._default_model is None:
            self._default_model = name

        print(f"âœ… OpenAI ëª¨ë¸ '{name}' ì¶”ê°€ë¨ (ëª¨ë¸: {model})")
        return llm

    def add_ollama_model(
        self,
        name: str = "ollama",
        model: str = "qwen2.5:7b-instruct-q4_K_M",
        base_url: str = None,
        temperature: float = 0.0,
        **kwargs
    ) -> BaseChatModel:
        """Ollama ëª¨ë¸ ì¶”ê°€"""
        # í™˜ê²½ë³€ìˆ˜ì—ì„œ Ollama í˜¸ìŠ¤íŠ¸ í™•ì¸
        import os
        if base_url is None:
            ollama_host = os.getenv("OLLAMA_HOST", "localhost:11434")
            if not ollama_host.startswith("http"):
                base_url = f"http://{ollama_host}"
            else:
                base_url = ollama_host

        llm = ChatOllama(
            model=model,
            base_url=base_url,
            temperature=temperature,
            **kwargs
        )

        self._models[name] = llm
        if self._default_model is None:
            self._default_model = name

        print(f"âœ… Ollama ëª¨ë¸ '{name}' ì¶”ê°€ë¨ (ëª¨ë¸: {model})")
        return llm

    def get_model(self, name: Optional[str] = None) -> BaseChatModel:
        """ëª¨ë¸ ê°€ì ¸ì˜¤ê¸°"""
        model_name = name or self._default_model
        if not model_name or model_name not in self._models:
            raise ValueError(f"ëª¨ë¸ '{model_name}'ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸: {list(self._models.keys())}")

        return self._models[model_name]

    def list_models(self) -> List[str]:
        """ë“±ë¡ëœ ëª¨ë¸ ëª©ë¡ ë°˜í™˜"""
        return list(self._models.keys())

    async def invoke_model(
        self,
        messages: List[BaseMessage],
        model_name: Optional[str] = None,
        **kwargs
    ) -> AIMessage:
        """ëª¨ë¸ í˜¸ì¶œ"""
        model = self.get_model(model_name)
        return await model.ainvoke(messages, **kwargs)

    async def invoke_with_system(
        self,
        system_prompt: str,
        user_message: str,
        model_name: Optional[str] = None,
        **kwargs
    ) -> str:
        """ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ì™€ ì‚¬ìš©ì ë©”ì‹œì§€ë¡œ ëª¨ë¸ í˜¸ì¶œ"""
        messages = [
            SystemMessage(content=system_prompt),
            HumanMessage(content=user_message)
        ]

        response = await self.invoke_model(messages, model_name, **kwargs)
        return response.content

    def supports_structured_output(self, model_name: Optional[str] = None) -> bool:
        model = self.get_model(model_name)
        return hasattr(model, "with_structured_output")

    async def invoke_structured_with_system(
        self,
        system_prompt: str,
        user_message: str,
        schema_model: Type[BaseModel],
        model_name: Optional[str] = None,
        **kwargs
    ):
        """
        Try to call the underlying model with structured output (Pydantic schema).
        Returns the parsed object on success, or None if unsupported / failed.
        """
        model = self.get_model(model_name)
        try:
            if not hasattr(model, "with_structured_output"):
                return None
            structured_llm = model.with_structured_output(schema_model)
            messages = [
                SystemMessage(content=system_prompt),
                HumanMessage(content=user_message)
            ]
            resp = await structured_llm.ainvoke(messages, **kwargs)
            return resp
        except Exception as e:
            print(f"âš ï¸ Structured output invoke ì‹¤íŒ¨: {e}")
            return None


# ì „ì—­ LLM ë§¤ë‹ˆì € ì¸ìŠ¤í„´ìŠ¤
_llm_manager: Optional[LLMManager] = None


def get_llm_manager() -> LLMManager:
    """LLM ë§¤ë‹ˆì € ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
    global _llm_manager
    if _llm_manager is None:
        _llm_manager = LLMManager()
    return _llm_manager


def setup_default_llms():
    """ê¸°ë³¸ LLM ì„¤ì •"""
    manager = get_llm_manager()

    # OpenAI ëª¨ë¸ ì„¤ì • ì‹œë„
    try:
        if os.getenv("OPENAI_API_KEY"):
            manager.add_openai_model()
            print("ğŸ¤– OpenAI ëª¨ë¸ì´ ê¸°ë³¸ìœ¼ë¡œ ì„¤ì •ë˜ì—ˆìŠµë‹ˆë‹¤.")
        else:
            print("âš ï¸ OPENAI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"âš ï¸ OpenAI ëª¨ë¸ ì„¤ì • ì‹¤íŒ¨: {e}")

    # Ollama ëª¨ë¸ ì„¤ì • ì‹œë„
    try:
        manager.add_ollama_model()
        print("ğŸ¦™ Ollama ëª¨ë¸ì´ ì¶”ê°€ë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"âš ï¸ Ollama ëª¨ë¸ ì„¤ì • ì‹¤íŒ¨: {e}")

    # ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ í™•ì¸
    available_models = manager.list_models()
    if not available_models:
        print("âŒ ì‚¬ìš© ê°€ëŠ¥í•œ LLMì´ ì—†ìŠµë‹ˆë‹¤.")
        print("ğŸ’¡ í•´ê²° ë°©ë²•:")
        print("   1. OPENAI_API_KEY í™˜ê²½ë³€ìˆ˜ ì„¤ì •")
        print("   2. Ollama ì„œë²„ ì‹¤í–‰ (ollama serve)")
    else:
        print(f"âœ… ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸: {available_models}")


async def test_llm_connection(model_name: Optional[str] = None) -> bool:
    """LLM ì—°ê²° í…ŒìŠ¤íŠ¸"""
    try:
        manager = get_llm_manager()
        response = await manager.invoke_with_system(
            system_prompt="You are a helpful assistant.",
            user_message="Hello! Please respond with 'Connection successful!'",
            model_name=model_name
        )

        if "successful" in response.lower():
            print(f"âœ… LLM ì—°ê²° í…ŒìŠ¤íŠ¸ ì„±ê³µ: {model_name or 'default'}")
            return True
        else:
            print(f"âš ï¸ LLM ì‘ë‹µì´ ì˜ˆìƒê³¼ ë‹¤ë¦„: {response}")
            return False

    except Exception as e:
        print(f"âŒ LLM ì—°ê²° í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        return False


# í¸ì˜ í•¨ìˆ˜ë“¤
async def call_llm(
    system_prompt: str,
    user_message: str,
    model_name: Optional[str] = None
) -> str:
    """ê°„ë‹¨í•œ LLM í˜¸ì¶œ"""
    manager = get_llm_manager()
    return await manager.invoke_with_system(system_prompt, user_message, model_name)


async def call_llm_with_messages(
    messages: List[BaseMessage],
    model_name: Optional[str] = None
) -> str:
    """ë©”ì‹œì§€ ë¦¬ìŠ¤íŠ¸ë¡œ LLM í˜¸ì¶œ"""
    manager = get_llm_manager()
    response = await manager.invoke_model(messages, model_name)
    return response.content
