"""
ì—ì´ì „íŠ¸ì˜ í•µì‹¬ ì˜ì‚¬ê²°ì • ë° ë„êµ¬ ì‹¤í–‰ ë…¸ë“œ êµ¬í˜„
"""

from typing import Dict, Any
from pydantic import BaseModel, Field
from agentq.state import AgentState, increment_loop_count, add_error, clear_error
from agentq.llm_utils import get_llm_manager
from agentq.prompt_utils import (
    get_prompt_builder, ScratchpadManager,
    extract_action_from_response, extract_critique_decision, clean_response,
    split_output_blocks, extract_commands_and_status, parse_command_line
)
from agentq.tools import get_tool_executor, WebTool
import re
import json

# ë¦´ë¦¬ì¦ˆ í† ê¸€/ê°€ë“œ
ENABLE_CRITIC = True
ENABLE_MCTS_LITE = True

#
# --- Critic ì¶œë ¥ íŒŒì‹± ìœ í‹¸ ---
class CriticItem(BaseModel):
    cmd: str = Field(..., description="One of the given candidate commands EXACTLY as provided.")
    score: float = Field(..., ge=0.0, le=1.0, description="Confidence 0..1")

class CriticList(BaseModel):
    items: list[CriticItem]
def _normalize_cmd(s: str) -> str:
    import re
    s = (s or "").strip()
    s = s.strip('`"\' ')
    s = re.sub(r"\s+", " ", s)
    return s

# Ollama JSON Schema for critic ranking
CRITIC_JSON_SCHEMA = {
    "oneOf": [
        {
            "type": "array",
            "items": {
                "type": "object",
                "additionalProperties": False,
                "required": ["cmd", "score"],
                "properties": {
                    "cmd": {"type": "string", "minLength": 1},
                    "score": {"type": "number", "minimum": 0.0, "maximum": 1.0}
                }
            }
        },
        {
            "type": "object",
            "additionalProperties": True,
            "properties": {
                "items": {
                    "type": "array",
                    "items": {
                        "type": "object",
                        "additionalProperties": False,
                        "required": ["cmd", "score"],
                        "properties": {
                            "cmd": {"type": "string", "minLength": 1},
                            "score": {"type": "number", "minimum": 0.0, "maximum": 1.0}
                        }
                    }
                }
            },
            "required": ["items"]
        }
    ]
}

def _extract_json_array(text: str):
    import json, re
    if not text:
        return None
    try:
        return json.loads(text)
    except Exception:
        pass
    try:
        m = re.search(r"\[[\s\S]*\]", text)
        if m:
            return json.loads(m.group(0))
    except Exception:
        return None
    return None


def _parse_critic_output(critic_out: str, candidates: list[str]) -> list[tuple[str, float]]:
    import re
    cand_norm = [_normalize_cmd(c) for c in candidates]
    parsed = _extract_json_array(critic_out)
    print(f"--- CRITIC PARSER: parsed = {parsed} ---")
    items: list[tuple[str, float]] = []

    def clamp01(x):
        try:
            v = float(x)
        except Exception:
            return 0.5
        return 0.0 if v < 0 else 1.0 if v > 1 else v

    # New: handle dict containers and unwrap single-object payloads
    if isinstance(parsed, dict):
        # unwrap common containers or single-object payloads
        if "items" in parsed and isinstance(parsed["items"], list):
            parsed = parsed["items"]
        elif "data" in parsed and isinstance(parsed["data"], list):
            parsed = parsed["data"]
        elif "choices" in parsed and isinstance(parsed["choices"], list):
            parsed = parsed["choices"]
        elif "result" in parsed and isinstance(parsed["result"], list):
            parsed = parsed["result"]
        elif ("cmd" in parsed or "command" in parsed or "action" in parsed or "candidate" in parsed or "text" in parsed):
            parsed = [parsed]

    if isinstance(parsed, list):
        # Fast path A: pure numeric array -> align by index
        try:
            if all(isinstance(x, (int, float)) for x in parsed):
                nums = [clamp01(x) for x in parsed]
                items = []
                for i, c in enumerate(cand_norm):
                    sc = nums[i] if i < len(nums) else 0.5
                    items.append((c, sc))
                # convert back to original text forms
                return [(candidates[i], items[i][1]) for i in range(len(candidates))]
        except Exception:
            pass

        # Fast path B: objects with {"index": int, "score": num}
        try:
            if all(isinstance(x, dict) and ("index" in x) for x in parsed):
                idx_map = {}
                for x in parsed:
                    try:
                        idx = int(x.get("index"))
                        sc = clamp01(x.get("score", 0.5))
                        idx_map[idx] = sc
                    except Exception:
                        continue
                items = []
                for i, c in enumerate(cand_norm):
                    sc = idx_map.get(i, 0.5)
                    items.append((c, sc))
                return [(candidates[i], items[i][1]) for i in range(len(candidates))]
        except Exception:
            pass

        for ent in parsed:
            if isinstance(ent, dict):
                cmd_key = next((k for k in ent.keys() if k.lower() in ("cmd","command","action","candidate","text")), None)
                score_key = next((k for k in ent.keys() if k.lower() in ("score","confidence","prob","p")), None)
                # Use only .get() access for dict fields
                if cmd_key:
                    cmd = _normalize_cmd(str(ent.get(cmd_key, "")))
                    sc = clamp01(ent.get(score_key, 0.5)) if score_key else 0.5
                    if cmd:
                        items.append((cmd, sc))
            elif isinstance(ent, list) and len(ent) >= 1:
                cmd = _normalize_cmd(str(ent[0]))
                sc = clamp01(ent[1]) if len(ent) >= 2 else 0.5
                if cmd:
                    items.append((cmd, sc))
            elif isinstance(ent, str):
                cmd = _normalize_cmd(ent)
                if cmd:
                    items.append((cmd, 0.5))

    scores_map: dict[str, float] = {}
    for i, c in enumerate(candidates):
        cn = cand_norm[i]
        score = None
        for cmd, sc in items:
            n = _normalize_cmd(cmd)
            if n == cn:
                score = sc; break
        if score is None:
            for cmd, sc in items:
                if _normalize_cmd(cmd).casefold() == cn.casefold():
                    score = sc; break
        if score is None:
            for cmd, sc in items:
                n = _normalize_cmd(cmd)
                if n and (n in cn or cn in n):
                    score = sc; break
        if score is None:
            score = 0.5
        scores_map[c] = float(score)
    # If no items matched, set uniform 0.5 for all candidates
    if not items:
        return [(c, 0.5) for c in candidates]
    return [(c, scores_map[c]) for c in candidates]


def _progress_fingerprint(state: AgentState) -> str:
    from hashlib import md5
    url = (state.get("current_url") or "")
    title = (state.get("page_title") or "")
    content = (state.get("page_content") or "")[:1000]
    return md5(f"{url}|{title}|{content}".encode("utf-8")).hexdigest()


async def plan_node(state: AgentState) -> Dict[str, Any]:
    """Plan ë…¸ë“œ: ì „ì²´ ê³„íš ìˆ˜ë¦½"""
    print("ğŸ“‹ Plan ë…¸ë“œ ì‹¤í–‰ ì¤‘...")

    try:
        # í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        prompt_builder = get_prompt_builder()
        system_prompt = prompt_builder.build_plan_prompt(state)

        # LLM í˜¸ì¶œ
        llm_manager = get_llm_manager()
        response = await llm_manager.invoke_with_system(
            system_prompt=system_prompt,
            user_message=f"Please create a plan for: {state['objective']}"
        )

        # ì‘ë‹µ ì •ë¦¬
        plan = clean_response(response)

        # ìƒíƒœ ì—…ë°ì´íŠ¸
        state["plan"] = plan

        # ìŠ¤í¬ë˜ì¹˜íŒ¨ë“œì— ì¶”ê°€
        state = ScratchpadManager.add_plan(state, plan)

        print(f"   ê³„íš ìˆ˜ë¦½ ì™„ë£Œ: {plan[:100]}...")
        return {"plan": plan}

    except Exception as e:
        error_msg = f"Plan ë…¸ë“œ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {str(e)}"
        print(f"âŒ {error_msg}")
        state = add_error(state, error_msg)
        return {"plan": "ê³„íš ìˆ˜ë¦½ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤."}


async def thought_node(state: AgentState) -> Dict[str, Any]:
    """Thought ë…¸ë“œ: í›„ë³´ ìƒì„± â†’ critic ì ìˆ˜í™” â†’ ìµœì¢… ì•¡ì…˜ ì„ íƒ"""
    print(f"ğŸ¤” Thought ë…¸ë“œ ì‹¤í–‰ ì¤‘... (ë£¨í”„ {state['loop_count'] + 1})")

    try:
        # ë£¨í”„ ì¹´ìš´íŠ¸ ì¦ê°€
        state = increment_loop_count(state)

        # ìµœì‹  DOM ìŠ¤ëƒ…ìƒ· í™•ë³´ (ì—†ì„ ë•Œë§Œ)
        if not state.get("page_content"):
            snap = await WebTool.extract_page_content()
            if snap.get("success") and isinstance(snap.get("data"), dict):
                d = snap["data"]
                state["current_url"] = d.get("url")
                state["page_title"] = d.get("title")
                state["page_content"] = (d.get("content") or "")[:500]
                state["observation"] = "í˜ì´ì§€ ë‚´ìš© ì¶”ì¶œ ì™„ë£Œ"

        prompt_builder = get_prompt_builder()
        llm_manager = get_llm_manager()

        action = None
        thought_text = ""
        cmds = []
        status = "CONTINUE"
        best_cmd = None
        scores = []
        critic_out = "" # Initialize critic_out

        if ENABLE_CRITIC:
            # í›„ë³´ ì»¤ë§¨ë“œ ìƒì„± (ë³µì¡í•œ í”„ë¡¬í”„íŠ¸ ì‚¬ìš©)
            system_prompt = prompt_builder.build_thought_prompt(state)
            resp = await llm_manager.invoke_with_system(
                system_prompt=system_prompt,
                user_message="Propose multiple candidate COMMANDS for the very next step."
            )
            raw = clean_response(resp)
            cmds, status = extract_commands_and_status(raw)
            thought_text = split_output_blocks(raw).get("THOUGHT", "")

            if cmds and ENABLE_MCTS_LITE:
                try:
                    critic_out = ""
                    critic_prompt = prompt_builder.build_critic_prompt(state)
                    # Include strict instruction that EVERY candidate must appear exactly once
                    critic_in = (
                        "You are a ranking critic. For EACH of the following candidate commands, "
                        "assign a confidence score in [0,1]. Include EVERY candidate exactly once; "
                        "the `cmd` field must be a verbatim copy.\n\n"
                        + "\n".join([f"- {c}" for i, c in enumerate(cmds)])
                    )

                    # --- 0) First try: enforce JSON Schema (Ollama structured outputs) ---
                    parsed_scores = None
                    critic_json_raw = None
                    try:
                        critic_json_raw = await llm_manager.invoke_json_schema_with_system(
                            critic_prompt,
                            critic_in,
                            CRITIC_JSON_SCHEMA,
                            model_name=None  # use default model
                        )
                    except Exception as _e0:
                        print(f"âš ï¸ Critic JSON-schema invoke ì˜¤ë¥˜: {_e0}")

                    if critic_json_raw:
                        try:
                            data = json.loads(critic_json_raw)
                            # Unwrap common containers or single-object payloads
                            if isinstance(data, dict):
                                if "items" in data and isinstance(data["items"], list):
                                    data = data["items"]
                                elif "data" in data and isinstance(data["data"], list):
                                    data = data["data"]
                                elif "choices" in data and isinstance(data["choices"], list):
                                    data = data["choices"]
                                elif "result" in data and isinstance(data["result"], list):
                                    data = data["result"]
                                elif any(k in data for k in ("cmd","command","action","candidate","text")):
                                    data = [data]
                            critic_out = critic_json_raw  # for logging
                            norm_map: dict[str, float] = {}
                            # Case 1: array of objects with cmd/score (ideal)
                            if isinstance(data, list) and all(isinstance(ent, dict) for ent in data):
                                for ent in data:
                                    ncmd = None
                                    sc = 0.5
                                    try:
                                        # tolerate variants, use only .get()
                                        for k in ("cmd", "command", "action", "candidate", "text"):
                                            if k in ent and isinstance(ent.get(k), (str, int, float)):
                                                ncmd = _normalize_cmd(str(ent.get(k)))
                                                break
                                        for k in ("score", "confidence", "prob", "p"):
                                            if k in ent:
                                                sc = float(ent.get(k))
                                                break
                                    except Exception:
                                        pass
                                    if ncmd:
                                        norm_map[ncmd] = max(0.0, min(1.0, sc))
                                if norm_map:
                                    parsed_scores = [(c, float(norm_map.get(_normalize_cmd(c), 0.5))) for c in cmds]
                            # Case 2: array of numbers -> align by index
                            if not parsed_scores and isinstance(data, list) and all(isinstance(x, (int, float)) for x in data):
                                nums = [max(0.0, min(1.0, float(x))) for x in data]
                                parsed_scores = [(c, nums[i] if i < len(nums) else 0.5) for i, c in enumerate(cmds)]
                            # Case 3: array of {"index": int, "score": num}
                            if not parsed_scores and isinstance(data, list) and all(isinstance(x, dict) and ("index" in x) for x in data):
                                idx_map = {}
                                for x in data:
                                    try:
                                        idx = int(x.get("index"))
                                        sc = max(0.0, min(1.0, float(x.get("score", 0.5))))
                                        idx_map[idx] = sc
                                    except Exception:
                                        continue
                                parsed_scores = [(c, idx_map.get(i, 0.5)) for i, c in enumerate(cmds)]
                        except Exception as _ejson:
                            print(f"âš ï¸ Critic JSON-schema íŒŒì‹± ì‹¤íŒ¨: {_ejson}")
                            parsed_scores = None

                    # --- 1) Try structured output (OpenAI etc.) ---
                    structured = None
                    if not parsed_scores:
                        try:
                            structured = await llm_manager.invoke_structured_with_system(
                                critic_prompt,
                                critic_in,
                                CriticList
                            )
                        except Exception as _e_struct:
                            print(f"âš ï¸ Critic structured invoke ì‹¤íŒ¨: {_e_struct}")
                            structured = None

                    # Case 1: Pydantic BaseModel ë¡œ ë°˜í™˜ëœ ê²½ìš°ë§Œ .items ì‚¬ìš©
                    from pydantic import BaseModel as _PBM
                    if (not parsed_scores) and isinstance(structured, _PBM):
                        try:
                            s_items = getattr(structured, "items", None)
                            if s_items:
                                tmp = [(_normalize_cmd(getattr(it, "cmd")), float(getattr(it, "score"))) for it in s_items]
                                norm_map = {cmd: sc for cmd, sc in tmp}
                                parsed_scores = [(c, float(norm_map.get(_normalize_cmd(c), 0.5))) for c in cmds]
                        except Exception as _e:
                            print(f"âš ï¸ Structured critic íŒŒì‹± ì˜¤ë¥˜(BaseModel): {_e}")
                            parsed_scores = None

                    # Case 2: dict/list/str ë¡œ ë°˜í™˜ë˜ë©´ í†µí•© íŒŒì„œì— ìœ„ì„
                    if (not parsed_scores) and isinstance(structured, (dict, list)):
                        try:
                            import json as _json
                            parsed_scores = _parse_critic_output(_json.dumps(structured), cmds)
                        except Exception as _e:
                            print(f"âš ï¸ Structured critic íŒŒì‹± ì˜¤ë¥˜(dict/list): {_e}")
                            parsed_scores = None
                    elif (not parsed_scores) and isinstance(structured, str):
                        try:
                            parsed_scores = _parse_critic_output(structured, cmds)
                        except Exception as _e:
                            print(f"âš ï¸ Structured critic íŒŒì‹± ì˜¤ë¥˜(str): {_e}")
                            parsed_scores = None

                    # --- 2) Fallback: unstructured JSON parsing + one repair retry ---
                    if not parsed_scores:
                        critic_out = await llm_manager.invoke_with_system(critic_prompt, critic_in)
                        parsed_scores = _parse_critic_output(critic_out, cmds)
                        need_retry = (not parsed_scores) or all(abs(s - 0.5) < 1e-6 for _, s in parsed_scores)
                        if need_retry:
                            repair_user = (
                                "Your previous output was INVALID. Return ONLY a JSON array where each item is:\n"
                                "{\"cmd\": \"<one of the given candidates EXACTLY>\", \"score\": <float 0..1>}\n"
                                "Do NOT include any explanation or code fences. Include EVERY candidate exactly once.\n\n"
                                "Candidates (copy verbatim):\n" + "\n".join([f"- {c}" for c in cmds])
                            )
                            critic_out2 = await llm_manager.invoke_with_system(critic_prompt, repair_user)
                            try:
                                parsed2 = _parse_critic_output(critic_out2, cmds)
                            except Exception:
                                parsed2 = None
                            if parsed2:
                                parsed_scores = parsed2
                                critic_out = critic_out2
                    try:
                        preview = (critic_out or "<empty>")[:600]
                        print("   Critic ì›ë³¸ ì¶œë ¥(ìµœì¢…):\n" + preview)
                    except Exception:
                        pass

                    if parsed_scores:
                        scores.extend(parsed_scores)
                        try:
                            sample = ", ".join([f"{c}={s:.2f}" for c, s in parsed_scores[:3]])
                            print("   Critic ì ìˆ˜ ìƒ˜í”Œ: " + sample)
                        except Exception:
                            pass

                except Exception as e:
                    print(f"âš ï¸ Critic/MCTS-lite ë¸”ë¡ ì˜¤ë¥˜: {e}")
                    # ì•ˆì „ í´ë°±: ë¹„ì›Œë‘ë©´ ì•„ë˜ ê· ë“± ë¶„ë°° í´ë°±ìœ¼ë¡œ ì´ì–´ì§‘ë‹ˆë‹¤.

            # Single consolidated fallback for empty scores and cmds
            if not scores and cmds:
                scores = [(c, 0.5) for c in cmds]

            if scores:
                # Q í†µê³„ì™€ ê²°í•© (ê°„ë‹¨í•œ UCB-lite)
                qstats = state.get("q_stats") or {}
                alpha = 0.5
                scored = []
                for c, s in scores:
                    q = qstats.get(c, {}).get("Q", 0.0)
                    n = qstats.get(c, {}).get("N", 0)
                    bonus = 0.1 if n == 0 else 0.0
                    total = alpha * s + (1 - alpha) * q + bonus
                    scored.append((total, c))
                scored.sort(reverse=True)
                try:
                    preview = ", ".join([f"{c}~{t:.2f}" for t, c in sorted(scored, reverse=True)[:3]])
                    print(f"   UCB-lite ìƒìœ„: {preview}")
                except Exception:
                    pass
                best_cmd = scored[0][1]
                action = parse_command_line(best_cmd)
            else:
                # Critic/MCTS-lite ì‹¤íŒ¨ ì‹œ Option A (ë‹¨ì¼ ì•¡ì…˜)ìœ¼ë¡œ í´ë°±
                print("âš ï¸ Critic/MCTS-lite ì‹¤íŒ¨ ë˜ëŠ” í›„ë³´ ì—†ìŒ. Option Aë¡œ í´ë°±í•©ë‹ˆë‹¤.")
                # Fallback to Option A logic
                system_prompt = prompt_builder.build_thought_prompt(state) # ì´ í”„ë¡¬í”„íŠ¸ëŠ” ì´ì œ ë‹¨ìˆœí™”ëœ ë²„ì „
                resp = await llm_manager.invoke_with_system(
                    system_prompt=system_prompt,
                    user_message="What should be the next action based on the current situation?"
                )
                thought_text = clean_response(resp)
                action = extract_action_from_response(thought_text) # ë‹¨ìˆœí™”ëœ íŒŒì„œ ì‚¬ìš©
        else: # ENABLE_CRITIC is False
            # Option A (ë‹¨ì¼ ì•¡ì…˜) í”„ë¡¬í”„íŠ¸ ì‚¬ìš©
            system_prompt = prompt_builder.build_thought_prompt(state) # ì´ í”„ë¡¬í”„íŠ¸ëŠ” ì´ì œ ë‹¨ìˆœí™”ëœ ë²„ì „
            resp = await llm_manager.invoke_with_system(
                system_prompt=system_prompt,
                user_message="What should be the next action based on the current situation?"
            )
            thought_text = clean_response(resp)
            action = extract_action_from_response(thought_text) # ë‹¨ìˆœí™”ëœ íŒŒì„œ ì‚¬ìš©

        # ìƒíƒœ ì—…ë°ì´íŠ¸
        state["thought"] = thought_text or "ë‹¤ìŒ í–‰ë™ì„ ê²°ì •í–ˆìŠµë‹ˆë‹¤."
        state["candidate_commands"] = cmds
        state["critic_scores"] = [s for _, s in scores] if scores else []
        state["status"] = status
        state["last_command"] = best_cmd if 'best_cmd' in locals() else None
        state["action"] = action

        # ìŠ¤í¬ë˜ì¹˜íŒ¨ë“œ ê¸°ë¡
        state = ScratchpadManager.add_thought(state, state["thought"])
        if action:
            state = ScratchpadManager.add_action(state, action)

        print(f"   ì‚¬ê³  ê³¼ì •: {state['thought'][:100]}...")
        if action:
            print(f"   ê³„íšëœ ì•¡ì…˜: {action.get('type')}")
        else:
            print("   ì•¡ì…˜ì´ ì¶”ì¶œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

        return {
            "thought": state["thought"],
            "action": action,
            "loop_count": state["loop_count"],
            "candidate_commands": cmds
        }
    except Exception as e:
        error_msg = f"Thought ë…¸ë“œ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {str(e)}"
        print(f"âŒ {error_msg}")
        state = add_error(state, error_msg)
        return {
            "thought": "ë‹¤ìŒ í–‰ë™ì„ ê²°ì •í•˜ëŠ”ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.",
            "action": None,
            "loop_count": state["loop_count"]
        }


async def action_node(state: AgentState) -> Dict[str, Any]:
    """Action ë…¸ë“œ: ì‹¤ì œ ì•¡ì…˜ ì‹¤í–‰"""
    print("âš¡ Action ë…¸ë“œ ì‹¤í–‰ ì¤‘...")

    try:
        action = state.get("action")

        if not action:
            observation = "ì‹¤í–‰í•  ì•¡ì…˜ì´ ì—†ìŠµë‹ˆë‹¤."
            print(f"   {observation}")
            state["observation"] = observation
            state = ScratchpadManager.add_observation(state, observation)
            return {"observation": observation}

        # ë„êµ¬ ì‹¤í–‰
        tool_executor = get_tool_executor()
        result = await tool_executor.execute_action(action)

        # ê´€ì°° ê²°ê³¼ êµ¬ì„±
        if result["success"]:
            observation = result["message"]
            if result.get("data"):
                if isinstance(result["data"], dict):
                    # í˜ì´ì§€ ì •ë³´ ì—…ë°ì´íŠ¸
                    if "url" in result["data"]:
                        state["current_url"] = result["data"]["url"]
                    if "title" in result["data"]:
                        state["page_title"] = result["data"]["title"]
                    if "content" in result["data"]:
                        state["page_content"] = result["data"]["content"][:500]  # ì²˜ìŒ 500ìë§Œ
                        observation += f"\ní˜ì´ì§€ ë‚´ìš©: {result['data']['content'][:200]}..."
                elif isinstance(result["data"], str):
                    observation += f"\nê²°ê³¼: {result['data'][:200]}..."
        else:
            observation = f"ì•¡ì…˜ ì‹¤íŒ¨: {result['message']}"

        # ìƒíƒœ ì—…ë°ì´íŠ¸
        state["observation"] = observation
        state = ScratchpadManager.add_observation(state, observation)

        # ì—ëŸ¬ ìƒíƒœ ì´ˆê¸°í™” (ì„±ê³µí•œ ê²½ìš°)
        if result["success"]:
            state = clear_error(state)

        print(f"   ì‹¤í–‰ ê²°ê³¼: {observation[:100]}...")
        return {"observation": observation}

    except Exception as e:
        error_msg = f"Action ë…¸ë“œ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {str(e)}"
        print(f"âŒ {error_msg}")
        observation = f"ì•¡ì…˜ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
        state["observation"] = observation
        state = add_error(state, error_msg)
        state = ScratchpadManager.add_observation(state, observation)
        return {"observation": observation}


async def explanation_node(state: AgentState) -> Dict[str, Any]:
    """Explanation ë…¸ë“œ: ê²°ê³¼ ì„¤ëª…"""
    print("ğŸ“ Explanation ë…¸ë“œ ì‹¤í–‰ ì¤‘...")

    try:
        # í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        prompt_builder = get_prompt_builder()
        system_prompt = prompt_builder.build_explanation_prompt(state)

        # LLM í˜¸ì¶œ
        llm_manager = get_llm_manager()
        response = await llm_manager.invoke_with_system(
            system_prompt=system_prompt,
            user_message="Please explain what happened and its significance."
        )

        # ì‘ë‹µ ì •ë¦¬
        explanation = clean_response(response)

        # ìƒíƒœ ì—…ë°ì´íŠ¸
        state["explanation"] = explanation
        state = ScratchpadManager.add_explanation(state, explanation)

        print(f"   ì„¤ëª…: {explanation[:100]}...")
        return {"explanation": explanation}

    except Exception as e:
        error_msg = f"Explanation ë…¸ë“œ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {str(e)}"
        print(f"âŒ {error_msg}")
        explanation = f"ê²°ê³¼ í•´ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
        state["explanation"] = explanation
        state = add_error(state, error_msg)
        return {"explanation": explanation}


async def critique_node(state: AgentState) -> Dict[str, Any]:
    """Critique ë…¸ë“œ: ì™„ë£Œ ì—¬ë¶€ íŒë‹¨"""
    print("ğŸ” Critique ë…¸ë“œ ì‹¤í–‰ ì¤‘...")

    try:
        # í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        prompt_builder = get_prompt_builder()
        system_prompt = prompt_builder.build_critique_prompt(state)

        # LLM í˜¸ì¶œ
        llm_manager = get_llm_manager()
        response = await llm_manager.invoke_with_system(
            system_prompt=system_prompt,
            user_message="Should we continue or is the task complete?"
        )

        # ì‘ë‹µ ì •ë¦¬
        critique = clean_response(response)

        # ì™„ë£Œ ì—¬ë¶€ ê²°ì •
        done = extract_critique_decision(response)

        # ---- ì§„í–‰ë„/ë£¨í”„ ê°€ë“œ ----
        loops = state.get("loop_count", 0)
        min_loops = state.get("min_loops", 3)

        prev_fp = state.get("last_progress_fingerprint")
        curr_fp = _progress_fingerprint(state)
        progress_gain = (prev_fp != curr_fp) and bool(curr_fp)

        if progress_gain:
            state["no_progress_streak"] = 0
            state["last_progress_fingerprint"] = curr_fp
        else:
            state["no_progress_streak"] = state.get("no_progress_streak", 0) + 1

        if loops < min_loops:
            done = False

        if state.get("no_progress_streak", 0) >= 2 and loops >= min_loops:
            done = True
            critique += "\nHeuristic: No progress for multiple steps â†’ stopping."

        # ë„ë©”ì¸ íœ´ë¦¬ìŠ¤í‹± (OpenTable)
        try:
            url = state.get("current_url") or ""
            text = (state.get("page_content") or "").lower()
            if "opentable.com" in url:
                success_keywords = ["reservation confirmed", "complete reservation", "you're all set"]
                if any(k in text for k in success_keywords):
                    done = True
                    critique += "\nHeuristic: OpenTable success indicators found."
        except Exception:
            pass

        # ê°„ë‹¨í•œ Q-í†µê³„ ì—…ë°ì´íŠ¸ (ì„¸ì…˜ ë‚´ì—ì„œë§Œ)
        if ENABLE_MCTS_LITE:
            try:
                cmd = state.get("last_command")
                if cmd:
                    stats = state.get("q_stats") or {}
                    ent = stats.get(cmd, {"Q": 0.0, "N": 0})
                    reward = 1.0 if done else (0.2 if progress_gain else 0.0)
                    ent["N"] += 1
                    ent["Q"] = ent["Q"] + (reward - ent["Q"]) / ent["N"]
                    stats[cmd] = ent
                    state["q_stats"] = stats
            except Exception:
                pass

        # ìµœëŒ€ ë£¨í”„ íšŸìˆ˜ ì²´í¬
        if state["loop_count"] >= state["max_loops"]:
            done = True
            critique += f"\nìµœëŒ€ ë£¨í”„ íšŸìˆ˜({state['max_loops']})ì— ë„ë‹¬í•˜ì—¬ ì¢…ë£Œí•©ë‹ˆë‹¤."

        print(f"   ë£¨í”„ {loops}, min_loops {min_loops}, no_progress_streak {state.get('no_progress_streak')}, done={done}")

        # ìƒíƒœ ì—…ë°ì´íŠ¸
        state["done"] = done
        state = ScratchpadManager.add_critique(state, critique, done)

        status = "ì™„ë£Œ" if done else "ê³„ì†"
        print(f"   í‰ê°€ ê²°ê³¼: {status} - {critique[:100]}...")

        return {
            "done": done,
            "critique": critique
        }

    except Exception as e:
        error_msg = f"Critique ë…¸ë“œ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {str(e)}"
        print(f"âŒ {error_msg}")

        # ì˜¤ë¥˜ ë°œìƒ ì‹œ ì•ˆì „í•˜ê²Œ ì¢…ë£Œ
        critique = f"í‰ê°€ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
        state["done"] = True
        state = add_error(state, error_msg)

        return {
            "done": True,
            "critique": critique
        }


# ë¼ìš°íŒ… í•¨ìˆ˜
def should_continue(state: AgentState) -> str:
    """ë‹¤ìŒ ë…¸ë“œ ê²°ì •"""
    if state.get("done", False):
        return "end"
    else:
        return "thought"


def check_max_loops(state: AgentState) -> str:
    """ìµœëŒ€ ë£¨í”„ íšŸìˆ˜ ì²´í¬"""
    if state["loop_count"] >= state["max_loops"]:
        return "critique"  # ê°•ì œë¡œ critiqueë¡œ ë³´ë‚´ì„œ ì¢…ë£Œ ì²˜ë¦¬
    return "action"
